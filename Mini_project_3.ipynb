{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mini-project 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbc0qpRLhPbg"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import time\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive' )\n",
        "%cd '/content/gdrive/MyDrive/mini-proj3_data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGGHMBkHuB2Y"
      },
      "source": [
        "# //////////////////////////////////////////////////////////////////////////////\n",
        "# ////////////////////////// Functions and Classes /////////////////////////////\n",
        "# //////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, img_file, label_file, transform=None, idx = None):\n",
        "        self.data = pickle.load( open( img_file, 'rb' ), encoding='bytes')\n",
        "        self.targets = np.genfromtxt(label_file, delimiter=',', skip_header=1)[:,1:]\n",
        "\n",
        "        if idx is not None:\n",
        "          self.targets = self.targets[idx]\n",
        "          self.data = self.data[idx]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], int(self.targets[index])\n",
        "        img = Image.fromarray(img.astype('uint8'), mode='L')\n",
        "\n",
        "        if self.transform is not None:\n",
        "           img = self.transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "\n",
        "class Net1(nn.Module):\n",
        "    # This part defines the layers\n",
        "    def __init__(self):\n",
        "        super(Net1, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 3, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(3, 6, kernel_size=5)\n",
        "\n",
        "        self.fc1 = nn.Linear(2262, 9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "\n",
        "        # This layer is an imaginary one. It simply states that we should see each member of x\n",
        "        # as a vector of 320 elements, instead of a tensor of 20x4x4 (Notice that 20*4*4=320)\n",
        "        x = x.view(-1, 2262)\n",
        "\n",
        "        # Feedforeward layers. Remember that fc1 is a layer that goes from 320 to 50 neurons\n",
        "        x = (self.fc1(x))\n",
        "\n",
        "        # We should put an appropriate activation for the output layer.\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "\n",
        "class Net2(nn.Module):\n",
        "    # This part defines the layers\n",
        "    def __init__(self):\n",
        "        super(Net2, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 3, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(3, 6, kernel_size=5)\n",
        "\n",
        "        self.fc1 = nn.Linear(2262, 9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = F.tanh(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.tanh(F.max_pool2d(self.conv2(x), 2))\n",
        "\n",
        "        # This layer is an imaginary one. It simply states that we should see each member of x\n",
        "        # as a vector of 320 elements, instead of a tensor of 20x4x4 (Notice that 20*4*4=320)\n",
        "        x = x.view(-1, 2262)\n",
        "\n",
        "        # Feedforeward layers. Remember that fc1 is a layer that goes from 320 to 50 neurons\n",
        "        x = (self.fc1(x))\n",
        "\n",
        "        # We should put an appropriate activation for the output layer.\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "\n",
        "class Net3(nn.Module):\n",
        "    # This part defines the layers\n",
        "    def __init__(self):\n",
        "        super(Net3, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=9)\n",
        "        self.conv2 = nn.Conv2d(6, 10, kernel_size=7)\n",
        "        self.conv3 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "\n",
        "        self.fc1 = nn.Linear(3220, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 300)\n",
        "        self.fc3 = nn.Linear(300, 9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        # This layer is an imaginary one. It simply states that we should see each member of x\n",
        "        # as a vector of 320 elements, instead of a tensor of 20x4x4 (Notice that 20*4*4=320)\n",
        "        x = x.view(-1, 3220)\n",
        "\n",
        "        # Feedforeward layers. Remember that fc1 is a layer that goes from 320 to 50 neurons\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # Output layer\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        # We should put an appropriate activation for the output layer.\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "\n",
        "\n",
        "class Net4(nn.Module):\n",
        "    # This part defines the layers\n",
        "    def __init__(self):\n",
        "        super(Net4, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=9)\n",
        "        self.conv2 = nn.Conv2d(6, 10, kernel_size=7)\n",
        "        self.conv3 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "\n",
        "        self.fc1 = nn.Linear(3220, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 300)\n",
        "        self.fc3 = nn.Linear(300, 9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = F.tanh(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.tanh(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        # This layer is an imaginary one. It simply states that we should see each member of x\n",
        "        # as a vector of 320 elements, instead of a tensor of 20x4x4 (Notice that 20*4*4=320)\n",
        "        x = x.view(-1, 3220)\n",
        "\n",
        "        # Feedforeward layers. Remember that fc1 is a layer that goes from 320 to 50 neurons\n",
        "        x = F.tanh(self.fc1(x))\n",
        "        x = F.tanh(self.fc2(x))\n",
        "\n",
        "        # Output layer\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        # We should put an appropriate activation for the output layer.\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "        \n",
        "\n",
        "def train(epoch):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = F.log_softmax(network(data))\n",
        "    loss = F.nll_loss(output, target) #negative log liklhood loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 20 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*batchsize) + ((epoch-1)*len(train_loader.dataset)))\n",
        "      torch.save({\n",
        "                  'Lepoch': epoch,\n",
        "                  'network_state_dict':network.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'Ltotal_time': total_time,\n",
        "                  'Ltrain_losses': train_losses,\n",
        "                  'Ltrain_counter':train_counter,\n",
        "                  'Ltest_losses':test_losses,\n",
        "                  'Lloss': loss}, './trainsaves/model.pth')\n",
        "      # torch.save(optimizer.state_dict(), './trainsaves/optimizer.pth')\n",
        "\n",
        "\n",
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    test_loader = valid_loader\n",
        "    for data, target in test_loader:\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      output = F.log_softmax(network(data))\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nValid set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhHIs4owuCgG"
      },
      "source": [
        "# //////////////////////////////////////////////////////////////////////////////\n",
        "# /////////////////////// Inputs and Initialization ////////////////////////////\n",
        "# //////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "batchsize = 64       # Specify the batch size\n",
        "if_validation = True  # Specify if you want to consider a part of training data as validation set\n",
        "if_shuffle = True     # Specify if you want to shuffle the training data before training process\n",
        "\n",
        "learningrate = 0.01\n",
        "moment = 0.9\n",
        "num_of_epochs = 50\n",
        "random_seed = 0\n",
        "\n",
        "if if_validation:\n",
        "  train_idx = range(0,50000)\n",
        "  valid_idx = range(50000,60000)\n",
        "else:\n",
        "  train_idx = None\n",
        "# //////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = MyDataset('./Train.pkl', './TrainLabels.csv', transform=img_transform, idx=train_idx)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=if_shuffle)\n",
        "\n",
        "if if_validation:\n",
        "  valid_dataset = MyDataset('./Train.pkl', './TrainLabels.csv', transform=img_transform, idx=valid_idx)\n",
        "  valid_loader = DataLoader(valid_dataset, batch_size=batchsize, shuffle=if_shuffle)\n",
        "else:\n",
        "  valid_loader = train_loader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M_5ODkyvUr7"
      },
      "source": [
        "# //////////////////////////////////////////////////////////////////////////////\n",
        "# ///////////////////////////// Learning Stage /////////////////////////////////\n",
        "# //////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "network = Net1().to(device)\n",
        "# network = models.resnet152().to(device)\n",
        "# network.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)\n",
        "# network.fc = nn.Linear(in_features=2048, out_features=9, bias=True).to(device)\n",
        "\n",
        "\n",
        "# optimizer = optim.SGD(network.parameters(), lr=learningrate, momentum=moment)\n",
        "optimizer = optim.Adam(network.parameters())\n",
        "\n",
        "total_time = []\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "\n",
        "for epoch in range(1, num_of_epochs+1):\n",
        "  start_time = time.clock()\n",
        "  train(epoch)\n",
        "  run_time = (time.clock() - start_time)\n",
        "  total_time.append(run_time)\n",
        "\n",
        "  test()\n",
        "\n",
        "mean_time = sum(total_time) / len(total_time)\n",
        "print('The mean run time for each epoch was:', mean_time, 'seconds')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77vqRHYRRIne"
      },
      "source": [
        "# //////////////////////////////////////////////////////////////////////////////\n",
        "# ////////////////////////// Continue saved work ///////////////////////////////\n",
        "# //////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "network = Net1().to(device)\n",
        "# network = models.resnet152().to(device)\n",
        "# network.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)\n",
        "# network.fc = nn.Linear(in_features=2048, out_features=9, bias=True).to(device)\n",
        "\n",
        "# optimizer = optim.SGD(network.parameters(), lr=learningrate, momentum=moment)\n",
        "optimizer = optim.Adam(network.parameters())\n",
        "\n",
        "# //////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "checkpoint = torch.load('./trainsaves/model.pth')\n",
        "\n",
        "network.load_state_dict(checkpoint['network_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "lastepoch = checkpoint['Lepoch']\n",
        "loss = checkpoint['Lloss']\n",
        "total_time = checkpoint['Ltotal_time']\n",
        "train_losses = checkpoint['Ltrain_losses']\n",
        "train_counter = checkpoint['Ltrain_counter']\n",
        "test_losses = checkpoint['Ltest_losses']\n",
        "\n",
        "# //////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "for epoch in range(lastepoch+1, lastepoch+1+2):\n",
        "  start_time = time.clock()\n",
        "  train(epoch)\n",
        "  run_time = (time.clock() - start_time)\n",
        "  total_time.append(run_time)\n",
        "\n",
        "  test()\n",
        "\n",
        "mean_time = sum(total_time) / len(total_time)\n",
        "print('The mean run time for each epoch was:', mean_time, 'seconds')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NYYgRThX_9_"
      },
      "source": [
        "# //////////////////////////////////////////////////////////////////////////////\n",
        "# /////// Prediction of test data and download the labels in CSV format ////////\n",
        "# //////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "# Here, the values of \"TestdummyLabels.csv\" is not used. Just put a random file with 10000 labels (it is important that the size be 10000)\n",
        "testdataset = MyDataset('./Test.pkl', './TestdummyLabels.csv', transform=img_transform, idx=None)\n",
        "testloader = DataLoader(testdataset, shuffle=False)\n",
        "\n",
        "j=1\n",
        "network.eval()\n",
        "y_guess = []\n",
        "with torch.no_grad():\n",
        "  for data, target in testloader:\n",
        "        data = data.to(device)\n",
        "        outputarray = F.log_softmax(network(data))\n",
        "        outputlabel = outputarray.data.max(1, keepdim=True)[1]\n",
        "        if j % 500 == 0:\n",
        "          # print(type(outputlabel.item()), type(outputlabel))\n",
        "          print('The prediction is done up to label index:', j)\n",
        "        j = j+1\n",
        "        y_guess.append(outputlabel.item())\n",
        "\n",
        "y_guess2 = [x+5 for x in y_guess] # This line adds the previously subtracted 5 to the labels\n",
        "y_guess_df = pd.DataFrame(y_guess2, columns=['class'])\n",
        "y_guess_df.to_csv('KaggleLabels.csv', columns=['class'])\n",
        "files.download(\"KaggleLabels.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}